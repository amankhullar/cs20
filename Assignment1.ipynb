{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amankhullar/cs20/blob/master/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vqr_p3mEyKG7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<center> <h1> CS20 Assignment 1 </h1> </center>\n",
        "## Problem 1: Op is all you need"
      ]
    },
    {
      "metadata": {
        "id": "nOhCzXVAyAqy",
        "colab_type": "code",
        "outputId": "7ec3c50b-0dbf-420c-8e08-3a18df23cb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVL'] = '2'\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JQ3Tq1rC-haq",
        "colab_type": "code",
        "outputId": "09472db7-3b18-4f89-b4ca-8f26e8d23ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "\n",
        "x = tf.random_uniform([])\n",
        "y = tf.random_uniform([])\n",
        "\n",
        "op = tf.cond(x > y, lambda : x + y, lambda : x - y)\n",
        "\n",
        "print(sess.run(op))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1196562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1gAl0OujIZcy",
        "colab_type": "code",
        "outputId": "69f80e7b-73a1-4c97-ed2e-25a4a4685c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x = tf.random_uniform(shape = [], minval = -1, maxval = 1, name = \"x\")\n",
        "y = tf.random_uniform(shape = [], minval = -1, maxval = 1, name = \"y\")\n",
        "\n",
        "# Using a tf.Tensor as a Python 'bool' is not allowed\n",
        "# out = tf.zeros(shape = [1])\n",
        "\n",
        "# if tf.less(x, y):\n",
        "#     out = x + y\n",
        "# elif tf.greater(x, y):\n",
        "#     out = x - y\n",
        "# else:\n",
        "#     out = 0\n",
        "    \n",
        "def f1():\n",
        "    return x + y\n",
        "\n",
        "def f2():\n",
        "    return x - y\n",
        "\n",
        "def f3():\n",
        "    return tf.zeros(shape = [1])\n",
        "\n",
        "out = tf.case({tf.less(x, y) : f1, tf.greater(x, y) : f2}, default = f3, name = 'output', exclusive = True)\n",
        "print(sess.run(out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.27322483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L9Bxa4-OK8Fq",
        "colab_type": "code",
        "outputId": "fb99d183-4322-4f4a-a3e7-95b905274ecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "x = tf.constant([[0, -2, -1], [0, 1, 2]])\n",
        "y = tf.zeros_like(x)\n",
        "\n",
        "out = tf.equal(x, y, name = 'equal_output')\n",
        "\n",
        "print(sess.run(out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ True False False]\n",
            " [ True False False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4v-aBXgFXN6q",
        "colab_type": "code",
        "outputId": "985a1178-bb62-4352-db72-b4e2b81dffd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "\n",
        "data = tf.constant([29.05088806,  27.61298943,  31.19073486,  29.35532951,\n",
        "                     30.97266006,  26.67541885,  38.08450317,  20.74983215,\n",
        "                     34.94445419,  34.45999146,  29.06485367,  36.01657104,\n",
        "                     27.88236427,  20.56035233,  30.20379066,  29.51215172,\n",
        "                     33.71149445,  28.59134293,  36.05556488,  28.66994858])\n",
        "\n",
        "out = tf.gather(data, tf.where(tf.greater(data, 30)))\n",
        "\n",
        "print(sess.run(out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[31.190735]\n",
            " [30.97266 ]\n",
            " [38.084503]\n",
            " [34.944454]\n",
            " [34.45999 ]\n",
            " [36.01657 ]\n",
            " [30.20379 ]\n",
            " [33.711494]\n",
            " [36.055565]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_u0teeS3Rwq5",
        "colab_type": "code",
        "outputId": "0015d7d6-d933-4fc8-fb56-eb84280c0344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Numpy method of creating the diagonal matrix\n",
        "\n",
        "# data = np.arange(1,7)\n",
        "# out = np.diag(data)\n",
        "\n",
        "# out_tf = tf.constant(out)\n",
        "\n",
        "# print(sess.run(out_tf))\n",
        "\n",
        "#Tensorflow method\n",
        "\n",
        "data = tf.range(start = 1, limit = 7, name = 'range')\n",
        "out = tf.linalg.diag(data, name = 'diagonal')\n",
        "\n",
        "print(sess.run(out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0 0]\n",
            " [0 2 0 0 0 0]\n",
            " [0 0 3 0 0 0]\n",
            " [0 0 0 4 0 0]\n",
            " [0 0 0 0 5 0]\n",
            " [0 0 0 0 0 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dxSajqtAXKmN",
        "colab_type": "code",
        "outputId": "17a22d97-89de-4740-8590-92be303a1ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data = tf.random_normal(shape = [10, 10], name = 'matrix')\n",
        "\n",
        "out = tf.matrix_determinant(data, name = 'determinant')\n",
        "\n",
        "print(sess.run(out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-66.201225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wSKrj871ZKo_",
        "colab_type": "code",
        "outputId": "9c496709-83c1-4988-806c-98f4a3c92c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x = tf.constant([5, 2, 3, 5, 10, 6, 2, 3, 4, 2, 1, 1, 0, 9])\n",
        "\n",
        "out, idx = tf.unique(x)\n",
        "\n",
        "print(sess.run(out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 5  2  3 10  6  4  1  0  9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YYYVD65hrYwg",
        "colab_type": "code",
        "outputId": "defc6a5e-4fc9-43bb-87d5-8e745d54c3e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x = tf.random_normal(shape = [300], name = 'x')\n",
        "y = tf.random_normal(shape = [300], name = 'y')\n",
        "\n",
        "out = tf.cond(tf.reduce_mean(x - y) < 0, lambda : tf.losses.mean_squared_error(labels = x, predictions = y), lambda : tf.reduce_sum(tf.abs(x - y)))\n",
        "\n",
        "print(sess.run(out))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.150824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lACvEqfS4EWc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "amMJD4AnOj2L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ]
    },
    {
      "metadata": {
        "id": "ROEOJYCIWOPH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import struct\n",
        "import urllib\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def huber_loss(labels, predictions, delta=14.0):\n",
        "    residual = tf.abs(labels - predictions)\n",
        "    def f1(): return 0.5 * tf.square(residual)\n",
        "    def f2(): return delta * residual - 0.5 * tf.square(delta)\n",
        "    return tf.cond(residual < delta, f1, f2)\n",
        "\n",
        "def safe_mkdir(path):\n",
        "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "def read_birth_life_data(filename):\n",
        "    \"\"\"\n",
        "    Read in birth_life_2010.txt and return:\n",
        "    data in the form of NumPy array\n",
        "    n_samples: number of samples\n",
        "    \"\"\"\n",
        "    text = open(filename, 'r').readlines()[1:]\n",
        "    data = [line[:-1].split('\\t') for line in text]\n",
        "    births = [float(line[1]) for line in data]\n",
        "    lifes = [float(line[2]) for line in data]\n",
        "    data = list(zip(births, lifes))\n",
        "    n_samples = len(data)\n",
        "    data = np.asarray(data, dtype=np.float32)\n",
        "    return data, n_samples\n",
        "\n",
        "def download_one_file(download_url, \n",
        "                    local_dest, \n",
        "                    expected_byte=None, \n",
        "                    unzip_and_remove=False):\n",
        "    \"\"\" \n",
        "    Download the file from download_url into local_dest\n",
        "    if the file doesn't already exists.\n",
        "    If expected_byte is provided, check if \n",
        "    the downloaded file has the same number of bytes.\n",
        "    If unzip_and_remove is True, unzip the file and remove the zip file\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_dest) or os.path.exists(local_dest[:-3]):\n",
        "        print('%s already exists' %local_dest)\n",
        "    else:\n",
        "        print('Downloading %s' %download_url)\n",
        "        local_file, _ = urllib.request.urlretrieve(download_url, local_dest)\n",
        "        file_stat = os.stat(local_dest)\n",
        "        if expected_byte:\n",
        "            if file_stat.st_size == expected_byte:\n",
        "                print('Successfully downloaded %s' %local_dest)\n",
        "                if unzip_and_remove:\n",
        "                    with gzip.open(local_dest, 'rb') as f_in, open(local_dest[:-3],'wb') as f_out:\n",
        "                        shutil.copyfileobj(f_in, f_out)\n",
        "                    os.remove(local_dest)\n",
        "            else:\n",
        "                print('The downloaded file has unexpected number of bytes')\n",
        "\n",
        "def download_mnist(path):\n",
        "    \"\"\" \n",
        "    Download and unzip the dataset mnist if it's not already downloaded \n",
        "    Download from http://yann.lecun.com/exdb/mnist\n",
        "    \"\"\"\n",
        "    safe_mkdir(path)\n",
        "    url = 'http://yann.lecun.com/exdb/mnist'\n",
        "    filenames = ['train-images-idx3-ubyte.gz',\n",
        "                'train-labels-idx1-ubyte.gz',\n",
        "                't10k-images-idx3-ubyte.gz',\n",
        "                't10k-labels-idx1-ubyte.gz']\n",
        "    expected_bytes = [9912422, 28881, 1648877, 4542]\n",
        "\n",
        "    for filename, byte in zip(filenames, expected_bytes):\n",
        "        download_url = os.path.join(url, filename)\n",
        "        local_dest = os.path.join(path, filename)\n",
        "        download_one_file(download_url, local_dest, byte, True)\n",
        "\n",
        "def parse_data(path, dataset, flatten):\n",
        "    if dataset != 'train' and dataset != 't10k':\n",
        "        raise NameError('dataset must be train or t10k')\n",
        "\n",
        "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
        "    with open(label_file, 'rb') as file:\n",
        "        _, num = struct.unpack(\">II\", file.read(8))\n",
        "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
        "        new_labels = np.zeros((num, 10))\n",
        "        new_labels[np.arange(num), labels] = 1\n",
        "    \n",
        "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
        "    with open(img_file, 'rb') as file:\n",
        "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
        "        imgs = imgs.astype(np.float32) / 255.0\n",
        "        if flatten:\n",
        "            imgs = imgs.reshape([num, -1])\n",
        "\n",
        "    return imgs, new_labels\n",
        "\n",
        "def read_mnist(path, flatten=True, num_train=55000):\n",
        "    \"\"\"\n",
        "    Read in the mnist dataset, given that the data is stored in path\n",
        "    Return two tuples of numpy arrays\n",
        "    ((train_imgs, train_labels), (test_imgs, test_labels))\n",
        "    \"\"\"\n",
        "    imgs, labels = parse_data(path, 'train', flatten)\n",
        "    indices = np.random.permutation(labels.shape[0])\n",
        "    train_idx, val_idx = indices[:num_train], indices[num_train:]\n",
        "    train_img, train_labels = imgs[train_idx, :], labels[train_idx, :]\n",
        "    val_img, val_labels = imgs[val_idx, :], labels[val_idx, :]\n",
        "    test = parse_data(path, 't10k', flatten)\n",
        "    return (train_img, train_labels), (val_img, val_labels), test\n",
        "\n",
        "def get_mnist_dataset(batch_size):\n",
        "    # Step 1: Read in data\n",
        "    mnist_folder = 'data/mnist'\n",
        "    download_mnist(mnist_folder)\n",
        "    train, val, test = read_mnist(mnist_folder, flatten=False)\n",
        "\n",
        "    # Step 2: Create datasets and iterator\n",
        "    train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "    train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
        "    train_data = train_data.batch(batch_size)\n",
        "\n",
        "    test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "    test_data = test_data.batch(batch_size)\n",
        "\n",
        "    return train_data, test_data\n",
        "    \n",
        "def show(image):\n",
        "    \"\"\"\n",
        "    Render a given numpy.uint8 2D array of pixel data.\n",
        "    \"\"\"\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w53EnFo0DAPI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensorboard for Colab"
      ]
    },
    {
      "metadata": {
        "id": "b9m36v4UDIRa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e4c47abf-001f-41e6-8cef-1a397d6c7f7e"
      },
      "cell_type": "code",
      "source": [
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://d5c26176.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rBh0jT5I5Tmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "uCl5x_az5N6B",
        "colab_type": "code",
        "outputId": "cfdc19f8-2bf7-486b-a484-9a67abfe95cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "#Defining the hyperparameters\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "n_epochs = 30\n",
        "n_train = 60000\n",
        "n_test = 10000\n",
        "\n",
        "#Step 1: Read in data\n",
        "mnist_folder = 'data/mnist'\n",
        "download_mnist(mnist_folder)\n",
        "train, val, test = read_mnist(mnist_folder, flatten = True)\n",
        "\n",
        "#Step 2: Create datasets and iterator\n",
        "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "train_data = train_data.shuffle(10000)\n",
        "train_data = train_data.batch(batch_size)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "test_data = test_data.batch(batch_size)\n",
        "\n",
        "#Create one iterator and initialize it with different datasets\n",
        "iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
        "img, label = iterator.get_next()\n",
        "\n",
        "train_init = iterator.make_initializer(train_data)\n",
        "test_init = iterator.make_initializer(test_data)\n",
        "\n",
        "#Step 3: create weights and bias\n",
        "w = tf.get_variable(name = 'weights', shape = [784, 10], initializer = tf.random_normal_initializer(mean = 0, stddev = 0.01))\n",
        "b = tf.get_variable(name = 'bias', shape = [1, 10], initializer = tf.zeros_initializer())\n",
        "\n",
        "#Step 4: build model\n",
        "logits = tf.matmul(img, w) + b\n",
        "\n",
        "#Step 5: define the loss function\n",
        "entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = label, logits = logits, name = 'entropy')\n",
        "loss = tf.reduce_mean(entropy, name = 'loss')\n",
        "\n",
        "#Step 6: define training operation\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "#Step 7: calculate accuracy with test set\n",
        "preds = tf.nn.softmax(logits)\n",
        "correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(label,1))\n",
        "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "\n",
        "writer = tf.summary.FileWriter('./Graph/logreg', tf.get_default_graph())\n",
        "with tf.Session() as sess:\n",
        "    start_time = time.time()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    #train the model number of epochs times\n",
        "    for i in range(n_epochs):\n",
        "        sess.run(train_init)\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        try:\n",
        "            while True:\n",
        "                _, l = sess.run([optimizer, loss])\n",
        "                total_loss += l\n",
        "                n_batches += 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
        "    print('Toal time : {0} seconds'.format(time.time() - start_time))\n",
        "        \n",
        "    # test the model\n",
        "    sess.run(test_init)\t\t\t# drawing samples from test_data\n",
        "    total_correct_preds = 0\n",
        "    try:\n",
        "        while True:\n",
        "            accuracy_batch = sess.run(accuracy)\n",
        "            total_correct_preds += accuracy_batch\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "\n",
        "    print('Accuracy {0}'.format(total_correct_preds/n_test))\n",
        "writer.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/mnist/train-images-idx3-ubyte.gz already exists\n",
            "data/mnist/train-labels-idx1-ubyte.gz already exists\n",
            "data/mnist/t10k-images-idx3-ubyte.gz already exists\n",
            "data/mnist/t10k-labels-idx1-ubyte.gz already exists\n",
            "Average loss epoch 0: 0.36396381806495576\n",
            "Average loss epoch 1: 0.29472465308946233\n",
            "Average loss epoch 2: 0.283943564791319\n",
            "Average loss epoch 3: 0.2781646569107854\n",
            "Average loss epoch 4: 0.2736546872660171\n",
            "Average loss epoch 5: 0.2742128196670566\n",
            "Average loss epoch 6: 0.2704728376900041\n",
            "Average loss epoch 7: 0.2655832806298899\n",
            "Average loss epoch 8: 0.2627160980777685\n",
            "Average loss epoch 9: 0.263606508318768\n",
            "Average loss epoch 10: 0.26379387650725455\n",
            "Average loss epoch 11: 0.2616857203633286\n",
            "Average loss epoch 12: 0.26098569477366845\n",
            "Average loss epoch 13: 0.2592407028688941\n",
            "Average loss epoch 14: 0.2581491239022377\n",
            "Average loss epoch 15: 0.25816859088318295\n",
            "Average loss epoch 16: 0.25917671565399614\n",
            "Average loss epoch 17: 0.2572768713325955\n",
            "Average loss epoch 18: 0.2567378666511802\n",
            "Average loss epoch 19: 0.255411796271801\n",
            "Average loss epoch 20: 0.25680455617433373\n",
            "Average loss epoch 21: 0.2562983932709971\n",
            "Average loss epoch 22: 0.2537426912853884\n",
            "Average loss epoch 23: 0.25393253959888634\n",
            "Average loss epoch 24: 0.2514078017844017\n",
            "Average loss epoch 25: 0.2532364749111408\n",
            "Average loss epoch 26: 0.2547773734081623\n",
            "Average loss epoch 27: 0.25270022446679513\n",
            "Average loss epoch 28: 0.24949949711214664\n",
            "Average loss epoch 29: 0.2521747361435447\n",
            "Toal time : 24.108951807022095 seconds\n",
            "Accuracy 0.9183\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}